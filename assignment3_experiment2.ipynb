{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussienalbared/Cudalab/blob/main/assignment3_experiment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "hf10RH1wD_60"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
        "from torchvision import transforms as tf, models, datasets\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import shutil\n",
        "\n",
        "writer = SummaryWriter()\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ['CUDA_VISIBLE_DEVICES']=\"0\" \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "gvKI9mhRSBxc"
      },
      "outputs": [],
      "source": [
        "alpha=0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfnfjPYPFenN",
        "outputId": "80e7fb77-cd09-4d11-c25c-136ef1da48ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive2/; to attempt to forcibly remount, call drive.mount(\"/content/drive2/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "mount_point=\"/content/drive2/\"\n",
        "drive.mount(mount_point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "tf6CrN6SPgRp"
      },
      "outputs": [],
      "source": [
        "tensord_board_dir=f\"drive2/MyDrive/tensorboard/tboard_logs/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "6BIoC-rFRU_g"
      },
      "outputs": [],
      "source": [
        "# TBOARD_LOGS = os.path.join(tensord_board_dir, \"test\")\n",
        "# !mkdir TBOARD_LOGS\n",
        "# if not os.path.exists(TBOARD_LOGS):\n",
        "#     os.makedirs(TBOARD_LOGS)\n",
        "# shutil.rmtree(TBOARD_LOGS)\n",
        "# writer = SummaryWriter(TBOARD_LOGS)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "yUVWt6f5Gnsh"
      },
      "outputs": [],
      "source": [
        "#!tar -xvzf drive2/MyDrive/annotations.tar.gz -C drive2/MyDrive/\n",
        "#!tar -xvzf drive2/MyDrive/images_cuda.tar.gz -C drive2/MyDrive/colab_unzip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "PBhyQwRLKwF7"
      },
      "outputs": [],
      "source": [
        "def mixup_data(x, y, alpha=1.0):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "   \n",
        "    index = torch.randperm(batch_size)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "7d-349UvK94L"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "WgxTzSziPgRx"
      },
      "outputs": [],
      "source": [
        "# Set the directory path\n",
        "#dir_path = '/home/user/albaredh0/project/images/'\n",
        "dir_path= \"drive2/MyDrive/colab_unzip/images\"\n",
        "annotations_path=\"drive2/MyDrive/annotations/list.txt\"\n",
        "df = pd.read_csv(annotations_path,\n",
        "                   skiprows=6,header=None,delimiter=r\"\\s+\",names=[\"ImageName\",\"CLASSID\",\"labels\",\"BREEDID\"])\n",
        "\n",
        "df[\"ImageName\"]=df[\"ImageName\"].apply(lambda x:x.strip())\n",
        "#df[\"labels\"]=df[\"labels\"]\n",
        "img_names=[ f.split(\".\")[0].strip() for f in os.listdir(dir_path) if f.split(\".\")[-1] in [\"jpg\",\"jpeg\"]] \n",
        "img_paths=[ os.path.join(dir_path, f) for f in  os.listdir(dir_path) if f.split(\".\")[-1] in [\"jpg\",\"jpeg\"]]\n",
        "text_labels_df = pd.DataFrame({'ImagesPath': img_paths, 'ImageName': img_names})\n",
        "merged_df = pd.merge(df, text_labels_df,on=\"ImageName\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "5-tAidNkPgRz"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "from torchvision.io import read_image,ImageReadMode\n",
        "\n",
        "class SplitedDataset(Dataset):\n",
        "    def __init__(self,dataFrame,transform): \n",
        "        self.data = dataFrame\n",
        "        y=self.data.iloc[:,2].values\n",
        "        x=self.data.iloc[:,4].values\n",
        "        self.transform=transform\n",
        "\n",
        "        self.x_train=x\n",
        "        self.y_train=torch.tensor(y,dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.data)    \n",
        "    def __getitem__(self, idx):\n",
        "        # _,_,_,_,img_path=self.data.iloc[idx]\n",
        "        img_path=self.x_train[idx]\n",
        "        # img = Image.open(img_path)\n",
        "        # img = np.array(img)\n",
        "        img = read_image(img_path,mode=ImageReadMode.RGB)\n",
        "        # .float()\n",
        "        # img = torch.Tensor(img)\n",
        "        # img = img.permute((2, 0, 1)) \n",
        "        if self.transform:\n",
        "            img = self.transform(img) \n",
        "        # ttt=transforms.Resize((150, 150))\n",
        "        # img=ttt(img)\n",
        "        label = self.y_train[idx]\n",
        "\n",
        "        # print(\"label img_path {} \".format(img_path))\n",
        "        return img,label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "L0g9lMcAPgR0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4c0f77-c774-4a2a-a03e-44e90ae916a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "64GW2c-EPgR1"
      },
      "outputs": [],
      "source": [
        "data_transforms = {\n",
        "    'train': tf.Compose([\n",
        "        tf.ToPILImage(), \n",
        "        tf.Resize(256),\n",
        "        tf.RandomResizedCrop(224),\n",
        "        tf.RandomHorizontalFlip(),\n",
        "        tf.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "        tf.ToTensor(),\n",
        "        tf.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': tf.Compose([\n",
        "        tf.ToPILImage(), \n",
        "        tf.Resize(256),\n",
        "        tf.CenterCrop(224),\n",
        "        tf.ToTensor(),\n",
        "        tf.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "ALp0urhLPgR2"
      },
      "outputs": [],
      "source": [
        "train_df=merged_df.sample(frac=0.7,random_state=16)\n",
        "valid_df=merged_df.drop(train_df.index)\n",
        "train_dataset=SplitedDataset(dataFrame=train_df,transform=data_transforms[\"train\"])\n",
        "# valid\n",
        "valid_dataset=SplitedDataset(dataFrame=valid_df,transform=data_transforms[\"test\"])\n",
        "\n",
        "train_loader=DataLoader(train_dataset,batch_size=128)\n",
        "\n",
        "test_loader=DataLoader(valid_dataset,batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "wGBrRR0KPgR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf92b803-d1d7-4db5-9469-6412dea3f93d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7349"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "len(valid_dataset)+len(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "749HffqXg9CQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34f2d0aa-d0d9-4e16-c134-cd43c0fdfe3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/home/user/albaredh0/project/src/dataset/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "#dataset_path=f'{mount_point}My Drive/colab_unzip/dataset/'\n",
        "\n",
        "dataset_path=f'/home/user/albaredh0/project/src/dataset/'\n",
        "dataset_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "uZvmZT6fPgR4"
      },
      "outputs": [],
      "source": [
        "# test=\"/home/user/albaredh0/project/images/Abyssinian_100.jpg\"\n",
        "# img = read_image(test)\n",
        "# qq=data_transforms[\"train\"]\n",
        "# qq(img).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "K578LOSHPgR4"
      },
      "outputs": [],
      "source": [
        "# train_dataset = datasets.ImageFolder(f'{dataset_path}train', data_transforms[\"train\"])\n",
        "# test_dataset = datasets.ImageFolder(f'{dataset_path}test', data_transforms[\"test\"])\n",
        "\n",
        "# N_train = len(train_dataset)\n",
        "# N_test = len(test_dataset)\n",
        "# print(f\"Training set size: {N_train} images\")\n",
        "# print(f\"Test set size: {N_test} images\")\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "# class_names = train_dataset.classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "zZizqbMyFMhx"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, epoch, device):\n",
        "    \"\"\" Training a model for one epoch \"\"\"\n",
        "    \n",
        "    loss_list = []\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        images, targets_a, targets_b, lam = mixup_data(images, labels,\n",
        "                                                       alpha)\n",
        "        \n",
        "        \n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "         \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
        "         \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        #loss = criterion(outputs, labels)\n",
        "        loss_list.append(loss.item())\n",
        "         \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "         \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "    mean_loss = np.mean(loss_list)\n",
        "    return mean_loss, loss_list\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, test_loader, criterion, device):\n",
        "    \"\"\" Evaluating the model for test \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_list = []\n",
        "    \n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass only to get logits/output\n",
        "        outputs = model(images)\n",
        "                 \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_list.append(loss.item())\n",
        "            \n",
        "        # Get predictions from the maximum value\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += len( torch.where(preds==labels)[0] )\n",
        "        total += len(labels)\n",
        "                 \n",
        "    # Total correct predictions and loss\n",
        "    accuracy = correct / total * 100\n",
        "    loss = np.mean(loss_list)\n",
        "    \n",
        "    return accuracy, loss\n",
        "\n",
        "\n",
        "def train_model(model, optimizer, scheduler, criterion, train_loader, valid_loader, num_epochs, tboard=None, start_epoch=0):\n",
        "    \"\"\" Training a model for a given number of epochs\"\"\"\n",
        "    \n",
        "    train_loss = []\n",
        "    val_loss =  []\n",
        "    loss_iters = []\n",
        "    valid_acc = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "           \n",
        "        # validation epoch\n",
        "        model.eval()  # important for dropout and batch norms\n",
        "        accuracy, loss = eval_model(\n",
        "                    model=model, test_loader=valid_loader,\n",
        "                    criterion=criterion, device=device\n",
        "            )\n",
        "        valid_acc.append(accuracy)\n",
        "        val_loss.append(loss)\n",
        "        writer.add_scalar(f'Accuracy/Valid', accuracy, global_step=epoch+start_epoch)\n",
        "        writer.add_scalar(f'Loss/Valid', loss, global_step=epoch+start_epoch)\n",
        "        \n",
        "        # training epoch\n",
        "        model.train()  # important for dropout and batch norms\n",
        "        mean_loss, cur_loss_iters = train_epoch(\n",
        "                model=model, train_loader=train_loader, optimizer=optimizer,\n",
        "                criterion=criterion, epoch=epoch, device=device\n",
        "            )\n",
        "        scheduler.step()\n",
        "        train_loss.append(mean_loss)\n",
        "        writer.add_scalar(f'Loss/Train', mean_loss, global_step=epoch+start_epoch)\n",
        "\n",
        "        loss_iters = loss_iters + cur_loss_iters\n",
        "        \n",
        "        if(epoch % 5 == 0 or epoch==num_epochs-1):\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "            print(f\"    Train loss: {round(mean_loss, 5)}\")\n",
        "            print(f\"    Valid loss: {round(loss, 5)}\")\n",
        "            print(f\"    Accuracy: {accuracy}%\")\n",
        "            print(\"\\n\")\n",
        "    \n",
        "    print(f\"Training completed\")\n",
        "    return train_loss, val_loss, loss_iters, valid_acc\n",
        "\n",
        "def smooth(f, K=5):\n",
        "    \"\"\" Smoothing a function using a low-pass filter (mean) of size K \"\"\"\n",
        "    kernel = np.ones(K) / K\n",
        "    f = np.concatenate([f[:int(K//2)], f, f[int(-K//2):]])  # to account for boundaries\n",
        "    smooth_f = np.convolve(f, kernel, mode=\"same\")\n",
        "    smooth_f = smooth_f[K//2: -K//2]  # removing boundary-fixes\n",
        "    return smooth_f\n",
        "\n",
        "def set_random_seed(random_seed=None):\n",
        "    \"\"\"\n",
        "    Using random seed for numpy and torch\n",
        "    \"\"\"\n",
        "    if(random_seed is None):\n",
        "        random_seed = 13\n",
        "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "    return\n",
        "\n",
        "set_random_seed()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "PS5XECESPgR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dfa0fb6-4251-43c6-f473-f4b9def0a5c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_number=37"
      ],
      "metadata": {
        "id": "VwLU7wEkkYfS"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "0dDoqPFRKeJJ"
      },
      "outputs": [],
      "source": [
        "ResNetFineTuned = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "num_nuerons = ResNetFineTuned.fc.in_features\n",
        "ResNetFineTuned.fc = nn.Linear(num_nuerons, class_number)\n",
        "ResNetFineTuned = ResNetFineTuned.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "QRr7ghcjVAe_"
      },
      "outputs": [],
      "source": [
        "def draw_plots(loss_iters,train_loss,val_loss,valid_acc):\n",
        "  plt.style.use('seaborn-v0_8')\n",
        "  fig, ax = plt.subplots(1,3)\n",
        "  fig.set_size_inches(24,5)\n",
        "\n",
        "  smooth_loss = smooth(loss_iters, 31)\n",
        "  ax[0].plot(loss_iters, c=\"blue\", label=\"Loss\", linewidth=3, alpha=0.5)\n",
        "  ax[0].plot(smooth_loss, c=\"red\", label=\"Smoothed Loss\", linewidth=3, alpha=1)\n",
        "  ax[0].legend(loc=\"best\")\n",
        "  ax[0].set_xlabel(\"Iteration\")\n",
        "  ax[0].set_ylabel(\"CE Loss\")\n",
        "  ax[0].set_title(\"Training Progress\")\n",
        "\n",
        "  epochs = np.arange(len(train_loss)) + 1\n",
        "  ax[1].plot(epochs, train_loss, c=\"red\", label=\"Train Loss\", linewidth=3)\n",
        "  ax[1].plot(epochs, val_loss, c=\"blue\", label=\"Valid Loss\", linewidth=3)\n",
        "  ax[1].legend(loc=\"best\")\n",
        "  ax[1].set_xlabel(\"Epochs\")\n",
        "  ax[1].set_ylabel(\"CE Loss\")\n",
        "  ax[1].set_title(\"Loss Curves\")\n",
        "\n",
        "  epochs = np.arange(len(val_loss)) + 1\n",
        "  ax[2].plot(epochs, valid_acc, c=\"red\", label=\"Valid accuracy\", linewidth=3)\n",
        "  ax[2].legend(loc=\"best\")\n",
        "  ax[2].set_xlabel(\"Epochs\")\n",
        "  ax[2].set_ylabel(\"Accuracy (%)\")\n",
        "  ax[2].set_title(f\"Valdiation Accuracy (max={round(np.max(valid_acc),2)}% @ epoch {np.argmax(valid_acc)+1})\")\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "EiRasvWhKK6e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_pretraind_model(model,model_name,num_epochs,optimizer=None):\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if not optimizer:\n",
        "     optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "    # Observe that all parameters are being optimized\n",
        "    \n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "    TBOARD_LOGS = os.path.join(tensord_board_dir, model_name)\n",
        "    if not os.path.exists(TBOARD_LOGS):\n",
        "        os.makedirs(TBOARD_LOGS)\n",
        "\n",
        "    shutil.rmtree(TBOARD_LOGS)\n",
        "    writer = SummaryWriter(TBOARD_LOGS)\n",
        "    train_loss, val_loss, loss_iters, valid_acc = train_model(\n",
        "            model=model, optimizer=optimizer, scheduler=scheduler, criterion=criterion,\n",
        "            train_loader=train_loader, valid_loader=test_loader, num_epochs=num_epochs,tboard=writer\n",
        "        )\n",
        "   \n",
        "    return train_loss, val_loss, loss_iters, valid_acc\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WwYSFRgJoGS"
      },
      "source": [
        "## Compare the following: Fine-Tuned ResNet, ResNet as fixed feature extractor, and ResNet with a Combined Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPWT2bUzJnvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b9d101-4118-4a07-a571-bdc36fc17f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "    Train loss: 0.66118\n",
            "    Valid loss: 3.37918\n",
            "    Accuracy: 6.4399092970521545%\n",
            "\n",
            "\n",
            "Epoch 6/20\n",
            "    Train loss: 0.23441\n",
            "    Valid loss: 0.0702\n",
            "    Accuracy: 99.36507936507937%\n",
            "\n",
            "\n",
            "Epoch 11/20\n",
            "    Train loss: 0.19054\n",
            "    Valid loss: 0.04389\n",
            "    Accuracy: 99.81859410430839%\n",
            "\n",
            "\n",
            "Epoch 16/20\n",
            "    Train loss: 0.17039\n",
            "    Valid loss: 0.04495\n",
            "    Accuracy: 99.72789115646259%\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_loss, val_loss, loss_iters, valid_acc =train_pretraind_model(ResNetFineTuned,model_name=\"ResNetFineTuned\",num_epochs=20)\n",
        "draw_plots(loss_iters=loss_iters,train_loss=train_loss,val_loss=val_loss,valid_acc=valid_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj9rZpXEJvmT"
      },
      "source": [
        "### ResNet as fixed feature extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzBGYIIqJwxB"
      },
      "outputs": [],
      "source": [
        "fixedModelresnet=models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "for param in fixedModelresnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_logits = fixedModelresnet.fc.in_features\n",
        "fixedModelresnet.fc = nn.Linear(num_logits, class_number)\n",
        "fixedModel = fixedModelresnet.to(device)\n",
        "train_loss, val_loss, loss_iters, valid_acc =train_pretraind_model(fixedModelresnet,model_name=\"fixedModelresnet\",num_epochs=20)\n",
        "draw_plots(loss_iters=loss_iters,train_loss=train_loss,val_loss=val_loss,valid_acc=valid_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0-ysUivJ5lH"
      },
      "source": [
        "### ResNet with a Combined Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePvdwxHiJ9Pe"
      },
      "outputs": [],
      "source": [
        "set_random_seed()\n",
        "\n",
        "combinedresnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT) #pretrained=True\n",
        "\n",
        "# Freezing model parameters\n",
        "for param in fixedModelresnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_logits = combinedresnet.fc.in_features\n",
        "combinedresnet.fc = nn.Linear(num_logits, class_number)\n",
        "combinedresnet = combinedresnet.to(device)\n",
        "train_loss, val_loss, loss_iters, valid_acc =train_pretraind_model(combinedresnet,\"CombinedApproachModelresnet\",num_epochs=10)\n",
        "\n",
        "\n",
        "\n",
        "# Unfreezing model parameters\n",
        "for param in combinedresnet.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(combinedresnet.parameters(), lr=3e-4)\n",
        "# Lowering learning rate\n",
        "for g in optimizer.param_groups:\n",
        "    g['lr'] *= 0.1\n",
        "\n",
        "next_train_loss, next_val_loss, next_loss_iters, next_valid_acc =train_pretraind_model(combinedresnet,\"CombinedApproachModelresnet\",num_epochs=10,optimizer=optimizer)\n",
        "concat_train_loss = train_loss + next_train_loss\n",
        "concat_val_loss = val_loss + next_val_loss\n",
        "concat_loss_iters = loss_iters + next_loss_iters\n",
        "concat_valid_acc = valid_acc + next_valid_acc\n",
        "draw_plots(loss_iters=concat_loss_iters,train_loss=concat_train_loss,val_loss=concat_val_loss,valid_acc=concat_valid_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fFuvZgqjZfT"
      },
      "source": [
        "#### Comparison between finetund resnet, feature extactor resnet,combined apparoach resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-r4CohXj06c"
      },
      "source": [
        "From the plot above the first difference between the three models IN the training, progress is approximately no difference between the combined approach and feature extractor approach and they are more smooth than the finetuned model.\n",
        "With respect to the loss curve, the feature extractor approach and the combined approach are similar with a small margin between the train loss and valid loss while in finetuned the model shows a fluctuation in both train and test loss.\n",
        "The same has happened with respect to validation accuracy the fine-tuned show some fluctuation in the latest epochs. \n",
        "In general the accuacy of the feature extractor  appraoch  anf the combined appraoch are approximitly the same with a small margin but the fine-tuned approach is the best in term of accuracy with a big marigin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1ZlVMUllixT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQRbBa99naq7"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "# %tensorboard --logdir logs\n",
        "%tensorboard --logdir \"/content/drive2/MyDrive/tboard_logs\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "V100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}